{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is our notebook for our web scraping code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we can add more information about the code we are **running**/*writing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librarys required for web scraping tools\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "#import timestring # this is what this has become in Python 3\n",
    "import time\n",
    "from time import strftime\n",
    "#from urllib import urlopen\n",
    "# The help us access the web pages\n",
    "import urllib3\n",
    "import certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise varialbes\n",
    "weeknum = []\n",
    "weekdatest = []\n",
    "weektype = []\n",
    "counter = 0\n",
    "allurl = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**set up all the fucntions required throughout the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geturl(usqurl):\n",
    "    http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())\n",
    "    page = http.request('GET', usqurl) # import the page we are interested in (e.g. dateusq)\n",
    "    soup = BeautifulSoup(page.data,\"lxml\")\n",
    "    #page=urllib3.urlopen(usqurl)   \n",
    "    #soup = BeautifulSoup(page,\"html\")\n",
    "    tablelnk=soup.find(\"table\", id=\"usqcoursetable\")                \n",
    "    for cxml in tablelnk.find_all('a'):\n",
    "        coursexml=str(cxml.get('href')) \n",
    "        allurl.append(usqurl+\"/\"+coursexml)\n",
    "    return allurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geturlcourse(cusqurl):\n",
    "    # page=urllib2.urlopen(cusqurl)\n",
    "    http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())\n",
    "    page = http.request('GET', cusqurl) # import the page we are interested in (e.g. dateusq)\n",
    "    soup = BeautifulSoup(page.data,\"lxml\")\n",
    "    tablelnk=soup.find('table', {'class':'usqoffertable'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what year are we interested in: \n",
    "yr = 2018 # this can be automated at a later time.\n",
    "\n",
    "# We need to set up the path for time tables.  \n",
    "usqpathtime='https://www.usq.edu.au/timetables/' \n",
    "# We need to update if the website changes\n",
    "# We need to set up the path for the course specifications.  \n",
    "usqpathspecs='http://www.usq.edu.au/course/specification/'\n",
    "# course synopsis\n",
    "usqpathgen='http://www.usq.edu.au/course/synopses/'\n",
    "# Read dates (e.g. start of semester) so we don't have to \n",
    "# type them in each semester/time we run the code\n",
    "dateusq = 'https://www.usq.edu.au/current-students/academic/academic-calendar'\n",
    "# To get the course specifications for specific years.\n",
    "usqurl=usqpathspecs+str(yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current offering we are investigating (can be automated)\n",
    "semno = '-S2-'#'-S3-'#'-S1-'\n",
    "semnum = ['-S1-','-S2-','-S3-','-FY']\n",
    "semmode = ['WEB','EXT','ONC'] # Anita these offine (e.g. online?)\n",
    "semonc = ['TWMBA','SPRNG','IPSCH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page = urllib3.urlopen(dateusqurl)\n",
    "http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())\n",
    "page = http.request('GET', dateusq) # import the page we are interested in (e.g. dateusq)\n",
    "soup = BeautifulSoup(page.data,\"lxml\")\n",
    "\n",
    "#print(soup) # returns the raw html for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# looking for specific information on the webpage\n",
    "tablelnk3=soup.find('table', attrs={'class':'table table-striped'})\n",
    "# print(tablelnk3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for td_tag in tablelnk3.find_all('td'):\n",
    "# print td_tag.string\n",
    "    if td_tag.string != None:\n",
    "    # print td_tag.string\n",
    "        if counter%3 == 0:\n",
    "            weeknum.append(td_tag.string)\n",
    "        elif counter%3 == 1:\n",
    "            weekdatest.append(td_tag.string) \n",
    "        elif counter%3 == 2:\n",
    "            weektype.append(td_tag.string)  \n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the strings\n",
    "# need to find the first week of semester! (or other interesting week)\n",
    "# worduniweek0 = weekdatest[0].split('-', 1 )[0]\n",
    "# print(worduniweek0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first week of semester 1: 26 Feb 2018\n",
      "computer format: \n",
      "2018-02-26 00:00:00\n",
      "Week of the calendar year: \n",
      "9\n"
     ]
    }
   ],
   "source": [
    "worduniweek0 = weekdatest[0].split('-', 1 )[0]\n",
    "worduniweek0 = worduniweek0.encode('ascii', 'ignore').decode('ascii')\n",
    "print('first week of semester 1: '+worduniweek0+' '+str(yr))\n",
    "     \n",
    "week0full=datetime.strptime(worduniweek0+' '+str(yr), '%d %b %Y')        \n",
    "print('computer format: ')\n",
    "print(week0full)\n",
    "        \n",
    "# fulldatewk0=datetime.strptime(week0full,\"%Y-%m-%d\")\n",
    "# check this every year\n",
    "startwk = datetime.date(week0full).isocalendar()[1]\n",
    "print('Week of the calendar year: ')\n",
    "print(startwk)\n",
    "# raise SystemExit(0) # <- kill switch for the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fulldatewk0 = datetime.strptime(week0full,\"%Y-%m-%d\")\n",
    "#print(fulldatewk0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the other start of the other semesters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the maximum week number each year\n",
    "maxwkno = int(strftime(\"%U\",time.strptime(\"31 Dec \"+str(yr), \"%d %b %Y\")))\n",
    "#print(maxwkno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using urllib3 to get pages\n",
    "#http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())\n",
    "#page = http.request('GET', dateusq) # import the page we are interested in (e.g. dateusq)\n",
    "#soup = BeautifulSoup(page.data,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "allurl=geturl(usqurl)\n",
    "# print(allurl)\n",
    "# type(allurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'urllib3' has no attribute 'urlopen'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-928b4cd045fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcurl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallurl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallurl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallurl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'urllib3' has no attribute 'urlopen'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is Elizabeth's code whichw we are modifying in the next block!\n",
    "\"\"\"\n",
    "\n",
    "# start cylcing through the urls\n",
    "#pn=[0,len(allurl)-1]\n",
    "\n",
    "#need to remove all the urllib2 referencs!!!! (Don't run yet!!!!!)\n",
    "# curl stands for counter for the URLs\n",
    "\n",
    "for curl in range(len(allurl)):   \n",
    "    page=urllib3.urlopen(allurl[curl]) \n",
    "    print(allurl[curl])\n",
    "    soup = BeautifulSoup(page,\"xml\") \n",
    "    # tablelnk=soup.find('table', {'class':'usqoffertable table'})     \n",
    "    tablelnk=soup.find('table', {'class':'table table-striped linkedtable'})\n",
    "    # print tablelnk\n",
    "    for cxml in tablelnk.find_all('a'): \n",
    "        rowshtml=str(cxml.get('href')) \n",
    "        htmlcode0=rowshtml.split('../../specification/'+str(yr)+'/', 1 )[1]\n",
    "        htmlcode1=htmlcode0.split('-', 1 )[0]\n",
    "        if oldcode==\"\": oldcode= htmlcode1 \n",
    "        newcode=htmlcode1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.usq.edu.au/course/specification/2018/../../synopses/2018/ACC1101.html\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'urlopen'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-8a65011d09e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallurl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mhttp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPoolManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcert_reqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CERT_REQUIRED'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mca_certs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcertifi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallurl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallurl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"soup: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'urlopen'"
     ]
    }
   ],
   "source": [
    "for curl in range(3):#len(allurl)):  \n",
    "    print(allurl[curl])\n",
    "    http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())\n",
    "    page = http.request('GET', allurl[curl])\n",
    "    soup = BeautifulSoup(allurl[curl],\"lxml\")\n",
    "    print(\"soup: \",soup)\n",
    "    tablelnk = soup.find('table', attrs={'class':'table table-striped'})\n",
    "    print(\"tablelnk: \",tablelnk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is where we are up to! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
